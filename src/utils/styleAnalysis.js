// æ–‡é£åˆ†æå·¥å…·

// æ¸…æ´—å†…å®¹ - ç§»é™¤æ— ç”¨çš„ç³»ç»Ÿæ–‡å­—å’Œå™ªéŸ³
export const cleanContent = (text) => {
  if (!text) return ''

  let cleaned = text

  // ç§»é™¤å¸¸è§çš„å…¬ä¼—å·ç³»ç»Ÿæç¤º
  const noisePatterns = [
    /ç‚¹å‡»ä¸Šæ–¹.*?å…³æ³¨/g,
    /ç‚¹å‡».*?è“å­—.*?å…³æ³¨/g,
    /å…³æ³¨.*?å…¬ä¼—å·/g,
    /é•¿æŒ‰.*?äºŒç»´ç .*?å…³æ³¨/g,
    /æ‰«æ.*?äºŒç»´ç .*?å…³æ³¨/g,
    /æ¨èé˜…è¯»/g,
    /å¾€æœŸ.*?å›é¡¾/g,
    /ç‚¹å‡».*?é˜…è¯»åŸæ–‡/g,
    /é˜…è¯»åŸæ–‡/g,
    /åŸæ–‡é“¾æ¥/g,
    /è½¬è½½è¯·æ³¨æ˜å‡ºå¤„/g,
    /ç‰ˆæƒå£°æ˜/g,
    /å…è´£å£°æ˜/g,
    /å•†åŠ¡åˆä½œ/g,
    /æŠ•ç¨¿é‚®ç®±/g,
    /è”ç³».*?å¾®ä¿¡/g,
    /æ·»åŠ .*?å¾®ä¿¡/g,
    /æ–‡ç« .*?æ¥æº/g,
    /.*?æ•´ç†.*?ç¼–è¾‘/g,
    /ç‚¹èµ.*?åœ¨çœ‹/g,
    /åˆ†äº«.*?ç‚¹èµ/g,
    /å–œæ¬¢.*?ç‚¹.*?åœ¨çœ‹/g,
    /é¢„è§ˆæ—¶æ ‡ç­¾ä¸å¯ç‚¹/g,
    /å¾®ä¿¡æ‰«ä¸€æ‰«/g,
    /ä½¿ç”¨å°ç¨‹åº/g,
    /å–æ¶ˆ/g,
    /å…è®¸/g,
    /å³å°†æ‰“å¼€.*?å°ç¨‹åº/g,
  ]

  noisePatterns.forEach(pattern => {
    cleaned = cleaned.replace(pattern, '')
  })

  // ç§»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™æ®µè½ç»“æ„ï¼‰
  cleaned = cleaned.replace(/\n\s*\n\s*\n/g, '\n\n')

  // ç§»é™¤é¦–å°¾ç©ºç™½
  cleaned = cleaned.trim()

  // ç§»é™¤è¿‡çŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯ç³»ç»Ÿæ–‡å­—ï¼Œå°‘äº5ä¸ªå­—ï¼‰
  cleaned = cleaned.split('\n')
    .filter(line => {
      const trimmed = line.trim()
      // ä¿ç•™ç©ºè¡Œï¼ˆæ®µè½åˆ†éš”ï¼‰æˆ–é•¿åº¦>=5çš„è¡Œ
      return trimmed.length === 0 || trimmed.length >= 5
    })
    .join('\n')

  return cleaned
}

// åˆ†è¯ - ç®€å•å®ç°ï¼ˆä¸­æ–‡æŒ‰å­—ç¬¦ï¼Œè‹±æ–‡æŒ‰å•è¯ï¼‰
export const tokenize = (text) => {
  // ç§»é™¤å¤šä½™ç©ºç™½
  text = text.replace(/\s+/g, ' ').trim()

  // åˆ†ç¦»ä¸­æ–‡å’Œè‹±æ–‡
  const tokens = []
  const chineseRegex = /[\u4e00-\u9fa5]/
  const words = text.split(/\s+/)

  words.forEach(word => {
    if (chineseRegex.test(word)) {
      // ä¸­æ–‡ï¼šæŒ‰å­—ç¬¦åˆ†è¯
      tokens.push(...word.split('').filter(c => chineseRegex.test(c)))
    } else {
      // è‹±æ–‡ï¼šä¿æŒå•è¯
      tokens.push(word.toLowerCase())
    }
  })

  return tokens
}

// æå–å…³é”®è¯ - åŸºäºè¯é¢‘
export const extractKeywords = (texts, topN = 20) => {
  const allText = texts.join(' ')
  const tokens = tokenize(allText)

  // æ‰©å±•çš„åœç”¨è¯åˆ—è¡¨
  const stopWords = new Set([
    // åŸºç¡€åœç”¨è¯
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™',
    'ä¸ª', 'ä»¬', 'ä¸º', 'èƒ½', 'ä»–', 'å¥¹', 'å®ƒ', 'å¤š', 'æ¥', 'å¹´', 'å¯¹', 'ä¸', 'åŠ', 'ä»¥', 'ç­‰', 'ä½†', 'æˆ–', 'è€Œ', 'ä¸­', 'é‡Œ', 'ä¸‹', 'å¤§', 'å°', 'ä¹ˆ', 'ç»™', 'ä»', 'æŠŠ', 'è¢«', 'è®©',
    'å‡º', 'å¯', 'ç”¨', 'æˆ', 'å› ', 'ä½œ', 'æ›´', 'è¿‡', 'è¿˜', 'ä¹‹', 'æ‰€', 'å¦‚', 'å…¶', 'åª', 'ä¸¤', 'ä¸‰', 'äº›', 'æœ€', 'å·²', 'äº', 'æ—¶', 'å', 'å‰', 'é—´', 'å¤©', 'æœˆ', 'æ—¥', 'æƒ³', 'å¾—',
    'ç§', 'ç‚¹', 'æ–¹', 'é¢', 'æ¬¡', 'ç°', 'å…³', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'å·²ç»', 'è¿˜æ˜¯', 'å¯ä»¥', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'çš„è¯', 'å¦‚æœ', 'è™½ç„¶',
    // å…¬ä¼—å·å¸¸è§è¯
    'æ–‡ç« ', 'å†…å®¹', 'é˜…è¯»', 'å…³æ³¨', 'åˆ†äº«', 'ç‚¹å‡»', 'æ‰«ç ', 'äºŒç»´ç ', 'å¾®ä¿¡', 'å…¬ä¼—å·', 'æ¨è', 'ç²¾å½©', 'åŸåˆ›', 'è½¬è½½', 'æ¥æº', 'ç¼–è¾‘', 'æ•´ç†', 'æ’ç‰ˆ', 'å›¾ç‰‡', 'è§†é¢‘',
    // æ•°å­—
    'ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹', 'å',
    // è‹±æ–‡åœç”¨è¯
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those', 'it', 'its', 'they', 'them', 'their'
  ])

  // ç»Ÿè®¡è¯é¢‘
  const freq = {}
  tokens.forEach(token => {
    if (!stopWords.has(token) && token.length > 1) {
      freq[token] = (freq[token] || 0) + 1
    }
  })

  // æ’åºå¹¶è¿”å›å‰ N ä¸ª
  return Object.entries(freq)
    .sort((a, b) => b[1] - a[1])
    .slice(0, topN)
    .map(([word, count]) => ({ word, count }))
}

// æå–å¸¸ç”¨çŸ­è¯­ï¼ˆ2-3 å­—/è¯ï¼‰
export const extractCommonPhrases = (texts, topN = 10) => {
  const allText = texts.join(' ')
  const sentences = allText.split(/[ã€‚ï¼ï¼Ÿ\n.!?]/).filter(s => s.trim())

  const phrases = {}
  const chineseRegex = /[\u4e00-\u9fa5]/

  sentences.forEach(sentence => {
    if (chineseRegex.test(sentence)) {
      // ä¸­æ–‡ï¼šæå– 2-3 å­—è¯ç»„
      for (let i = 0; i < sentence.length - 1; i++) {
        for (let len = 2; len <= 3 && i + len <= sentence.length; len++) {
          const phrase = sentence.slice(i, i + len)
          if (phrase.trim().length === len) {
            phrases[phrase] = (phrases[phrase] || 0) + 1
          }
        }
      }
    } else {
      // è‹±æ–‡ï¼šæå– 2-3 è¯ç»„
      const words = sentence.split(/\s+/).filter(w => w.length > 0)
      for (let i = 0; i < words.length - 1; i++) {
        for (let len = 2; len <= 3 && i + len <= words.length; len++) {
          const phrase = words.slice(i, i + len).join(' ')
          phrases[phrase] = (phrases[phrase] || 0) + 1
        }
      }
    }
  })

  return Object.entries(phrases)
    .filter(([_, count]) => count >= 2) // è‡³å°‘å‡ºç° 2 æ¬¡
    .sort((a, b) => b[1] - a[1])
    .slice(0, topN)
    .map(([phrase, count]) => ({ phrase, count }))
}

// è®¡ç®—å¹³å‡å¥å­é•¿åº¦
export const calculateAvgSentenceLength = (texts) => {
  const allText = texts.join(' ')
  const sentences = allText.split(/[ã€‚ï¼ï¼Ÿ\n.!?]/).filter(s => s.trim())

  if (sentences.length === 0) return 0

  const totalLength = sentences.reduce((sum, s) => sum + s.trim().length, 0)
  return Math.round(totalLength / sentences.length)
}

// åˆ†ææ ‡ç‚¹ç¬¦å·ä½¿ç”¨ä¹ æƒ¯
export const analyzePunctuation = (texts) => {
  const allText = texts.join(' ')
  const punctuations = {
    'ã€‚': 0, 'ï¼': 0, 'ï¼Ÿ': 0, 'ï¼Œ': 0, 'ï¼›': 0, 'ï¼š': 0,
    '.': 0, '!': 0, '?': 0, ',': 0, ';': 0, ':': 0,
    'ã€': 0, 'â€¦': 0, 'â€”': 0, '"': 0, '"': 0, '\u2018': 0, '\u2019': 0
  }

  for (const char of allText) {
    if (char in punctuations) {
      punctuations[char]++
    }
  }

  return punctuations
}

// æ£€æµ‹è¯­æ°”/é£æ ¼
export const detectTone = (texts) => {
  const allText = texts.join(' ')

  // ç®€å•çš„è§„åˆ™æ£€æµ‹
  const casual = /å“ˆå“ˆ|å˜¿|å–‚|å“|å•Š|å‘€|å˜›|å§|å‘¢|å“¦|emmm|hhh/gi
  const formal = /å› æ­¤|æ‰€ä»¥|ç»¼ä¸Š|æ€»ä¹‹|ä»è€Œ|é‰´äº|åŸºäº/gi
  const humorous = /ç¬‘|å“­|å°´å°¬|æç¬‘|æœ‰è¶£|å“ˆå“ˆ/gi

  const casualCount = (allText.match(casual) || []).length
  const formalCount = (allText.match(formal) || []).length
  const humorousCount = (allText.match(humorous) || []).length

  if (casualCount > formalCount && casualCount > humorousCount) {
    return 'casual'
  } else if (formalCount > casualCount && formalCount > humorousCount) {
    return 'formal'
  } else if (humorousCount > 0) {
    return 'humorous'
  } else {
    return 'neutral'
  }
}

// åˆ†ææ®µè½å¼€å¤´æ–¹å¼
export const analyzeOpeningPatterns = (texts) => {
  const patterns = {
    question: 0,      // é—®å¥å¼€å¤´
    story: 0,         // æ•…äº‹/åœºæ™¯å¼€å¤´
    statement: 0,     // è§‚ç‚¹/é™ˆè¿°å¼€å¤´
    quote: 0,         // å¼•ç”¨å¼€å¤´
    data: 0           // æ•°æ®/äº‹å®å¼€å¤´
  }

  const examples = {
    question: [],
    story: [],
    statement: []
  }

  texts.forEach(text => {
    const paragraphs = text.split(/\n+/).filter(p => p.trim().length > 20)

    paragraphs.slice(0, 3).forEach(para => {
      const firstSentence = para.trim().split(/[ã€‚ï¼ï¼Ÿ.!?]/)[0]
      if (!firstSentence) return

      if (firstSentence.includes('ï¼Ÿ') || firstSentence.includes('?')) {
        patterns.question++
        if (examples.question.length < 3) examples.question.push(firstSentence.slice(0, 30))
      } else if (/^(é‚£å¤©|æœ‰ä¸€æ¬¡|è®°å¾—|æ›¾ç»|æœ€è¿‘|å»å¹´|æ˜¨å¤©|å‰å‡ å¤©)/.test(firstSentence)) {
        patterns.story++
        if (examples.story.length < 3) examples.story.push(firstSentence.slice(0, 30))
      } else if (/^(æˆ‘|æˆ‘ä»¬|æˆ‘è§‰å¾—|æˆ‘è®¤ä¸º|æˆ‘æƒ³|å…¶å®|è¯´å®è¯)/.test(firstSentence)) {
        patterns.statement++
        if (examples.statement.length < 3) examples.statement.push(firstSentence.slice(0, 30))
      }
    })
  })

  return { patterns, examples }
}

// åˆ†æè½¬æŠ˜å’Œè¿æ¥æ–¹å¼
export const analyzeTransitions = (texts) => {
  const transitions = {
    'ä½†æ˜¯': 0, 'ä½†': 0, 'ç„¶è€Œ': 0, 'ä¸è¿‡': 0, 'å¯æ˜¯': 0,
    'æ‰€ä»¥': 0, 'å› æ­¤': 0, 'äºæ˜¯': 0,
    'è€Œä¸”': 0, 'å¹¶ä¸”': 0, 'åŒæ—¶': 0,
    'å…¶å®': 0, 'äº‹å®ä¸Š': 0, 'å®é™…ä¸Š': 0,
    'æ¢å¥è¯è¯´': 0, 'ä¹Ÿå°±æ˜¯è¯´': 0, 'æ¯”å¦‚': 0, 'ä¾‹å¦‚': 0
  }

  const allText = texts.join(' ')
  Object.keys(transitions).forEach(word => {
    const regex = new RegExp(word, 'g')
    const matches = allText.match(regex)
    if (matches) transitions[word] = matches.length
  })

  // æ’åºæ‰¾å‡ºæœ€å¸¸ç”¨çš„è½¬æŠ˜è¯
  const topTransitions = Object.entries(transitions)
    .filter(([_, count]) => count > 0)
    .sort((a, b) => b[1] - a[1])
    .slice(0, 5)

  return topTransitions
}

// åˆ†æäººç§°è§†è§’
export const analyzePerspective = (texts) => {
  const allText = texts.join(' ')
  const firstPerson = (allText.match(/æˆ‘|æˆ‘ä»¬|å’±ä»¬/g) || []).length
  const secondPerson = (allText.match(/ä½ |æ‚¨|ä½ ä»¬/g) || []).length
  const thirdPerson = (allText.match(/ä»–|å¥¹|å®ƒ|ä»–ä»¬|å¥¹ä»¬/g) || []).length

  const total = firstPerson + secondPerson + thirdPerson

  return {
    firstPerson: ((firstPerson / total) * 100).toFixed(1),
    secondPerson: ((secondPerson / total) * 100).toFixed(1),
    thirdPerson: ((thirdPerson / total) * 100).toFixed(1),
    dominant: firstPerson > secondPerson && firstPerson > thirdPerson ? 'first' :
              secondPerson > firstPerson && secondPerson > thirdPerson ? 'second' : 'third'
  }
}

// åˆ†æå¥å¼å¤æ‚åº¦å’Œå¤šæ ·æ€§
export const analyzeSentenceComplexity = (texts) => {
  const allText = texts.join(' ')
  const sentences = allText.split(/[ã€‚ï¼ï¼Ÿ.!?]/).filter(s => s.trim().length > 5)

  let simpleCount = 0    // ç®€å•å¥ï¼ˆå°‘äº15å­—ï¼Œæ— é€—å·ï¼‰
  let compoundCount = 0  // å¤åˆå¥ï¼ˆæœ‰é€—å·/åˆ†å·ï¼‰
  let complexCount = 0   // å¤æ‚å¥ï¼ˆå¤šä¸ªä»å¥ï¼‰

  sentences.forEach(sentence => {
    const length = sentence.length
    const commas = (sentence.match(/ï¼Œ|,|ï¼›|;/g) || []).length

    if (length < 15 && commas === 0) {
      simpleCount++
    } else if (commas >= 3) {
      complexCount++
    } else {
      compoundCount++
    }
  })

  const total = sentences.length
  return {
    simple: ((simpleCount / total) * 100).toFixed(1),
    compound: ((compoundCount / total) * 100).toFixed(1),
    complex: ((complexCount / total) * 100).toFixed(1),
    diversity: compoundCount > simpleCount && compoundCount > complexCount ? 'varied' :
               complexCount > simpleCount ? 'complex' : 'simple'
  }
}

// ä¸»åˆ†æå‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼‰
export const analyzeWritingStyle = (sources) => {
  if (!sources || sources.length === 0) {
    return null
  }

  const texts = sources.map(s => s.content).filter(Boolean)

  if (texts.length === 0) {
    return null
  }

  // åŸºç¡€åˆ†æï¼ˆä¿ç•™ï¼‰
  const keywords = extractKeywords(texts)
  const commonPhrases = extractCommonPhrases(texts)
  const avgSentenceLength = calculateAvgSentenceLength(texts)
  const punctuationStyle = analyzePunctuation(texts)
  const tone = detectTone(texts)
  const totalWords = texts.reduce((sum, text) => sum + text.length, 0)

  // æ·±åº¦åˆ†æï¼ˆæ–°å¢ï¼‰
  const openingPatterns = analyzeOpeningPatterns(texts)
  const transitions = analyzeTransitions(texts)
  const perspective = analyzePerspective(texts)
  const complexity = analyzeSentenceComplexity(texts)

  return {
    // åŸºç¡€æ•°æ®
    keywords,
    commonPhrases,
    avgSentenceLength,
    punctuationStyle,
    tone,
    totalWords,

    // æ·±åº¦åˆ†æ
    openingPatterns,
    transitions,
    perspective,
    complexity,

    analyzedAt: new Date().toISOString()
  }
}

// ç”Ÿæˆæ–‡é£æè¿°ï¼ˆç”¨äº AI promptï¼‰
export const generateStyleDescription = (analysis) => {
  if (!analysis) {
    return ''
  }

  const toneMap = {
    casual: 'è½»æ¾éšæ„',
    formal: 'æ­£å¼ä¸¥è°¨',
    humorous: 'å¹½é»˜é£è¶£',
    neutral: 'ä¸­æ€§å®¢è§‚'
  }

  const perspectiveMap = {
    first: 'ç¬¬ä¸€äººç§°ä¸ºä¸»ï¼ˆæˆ‘/æˆ‘ä»¬ï¼‰ï¼Œå¼ºè°ƒä¸ªäººä½“éªŒå’Œä¸»è§‚æ„Ÿå—',
    second: 'ç¬¬äºŒäººç§°ä¸ºä¸»ï¼ˆä½ /æ‚¨ï¼‰ï¼Œç›´æ¥ä¸è¯»è€…å¯¹è¯',
    third: 'ç¬¬ä¸‰äººç§°ä¸ºä¸»ï¼Œå®¢è§‚å™è¿°'
  }

  const complexityMap = {
    simple: 'åçˆ±çŸ­å¥ï¼Œç®€æ´ç›´æ¥',
    varied: 'å¥å¼å¯Œæœ‰å˜åŒ–ï¼Œé•¿çŸ­ç»“åˆ',
    complex: 'å–„ç”¨å¤æ‚å¥å¼ï¼Œè¡¨è¾¾å±‚æ¬¡ä¸°å¯Œ'
  }

  // åˆ†æå¥å­é•¿åº¦åå¥½
  const sentenceLengthStyle = analysis.avgSentenceLength < 15
    ? 'åå¥½ä½¿ç”¨çŸ­å¥ï¼ŒèŠ‚å¥æ˜å¿«ï¼Œé€‚åˆè¡¨è¾¾æ¸…æ™°çš„è§‚ç‚¹'
    : analysis.avgSentenceLength < 25
    ? 'å¥å­é•¿åº¦é€‚ä¸­ï¼Œå…¼é¡¾è¡¨è¾¾æ·±åº¦å’Œé˜…è¯»æµç•…æ€§'
    : 'å€¾å‘ä½¿ç”¨é•¿å¥ï¼Œå–„äºé“ºé™ˆå’Œç»†èŠ‚æå†™ï¼Œè¡¨è¾¾æ›´æœ‰å±‚æ¬¡æ„Ÿ'

  // åˆ†ææ ‡ç‚¹ä½¿ç”¨ä¹ æƒ¯
  const punc = analysis.punctuationStyle
  const totalPunc = Object.values(punc).reduce((sum, count) => sum + count, 0)
  const exclamationRatio = ((punc['ï¼'] + punc['!']) / totalPunc * 100).toFixed(1)
  const questionRatio = ((punc['ï¼Ÿ'] + punc['?']) / totalPunc * 100).toFixed(1)
  const ellipsisRatio = ((punc['â€¦'] || 0) / totalPunc * 100).toFixed(1)

  let punctuationStyle = ''
  if (parseFloat(exclamationRatio) > 5) {
    punctuationStyle += 'ç»å¸¸ä½¿ç”¨æ„Ÿå¹å·ï¼Œè¡¨è¾¾æƒ…æ„Ÿä¸°å¯Œã€æ€åº¦é²œæ˜ï¼›'
  }
  if (parseFloat(questionRatio) > 3) {
    punctuationStyle += 'å–„ç”¨é—®å¥ä¸è¯»è€…äº’åŠ¨ï¼Œå¼•å‘æ€è€ƒï¼›'
  }
  if (parseFloat(ellipsisRatio) > 2) {
    punctuationStyle += 'ä½¿ç”¨çœç•¥å·è¥é€ ç•™ç™½å’Œæ€è€ƒç©ºé—´ï¼›'
  }
  if (!punctuationStyle) {
    punctuationStyle = 'æ ‡ç‚¹ä½¿ç”¨å…‹åˆ¶ï¼Œä»¥é™ˆè¿°ä¸ºä¸»ï¼Œè¯­æ°”å¹³ç¨³ã€‚'
  }

  // æå–å…³é”®ä¸»é¢˜è¯
  const topKeywords = analysis.keywords.slice(0, 10).map(k => k.word)
  const themeWords = topKeywords.slice(0, 5).join('ã€')

  // æå–å¸¸ç”¨è¡¨è¾¾
  const topPhrases = analysis.commonPhrases.slice(0, 8).map(p => p.phrase)
  const expressionExamples = topPhrases.length > 0
    ? `\n\nã€å¸¸ç”¨è¡¨è¾¾æ–¹å¼ã€‘\n${topPhrases.map((p, i) => `${i + 1}. "${p}"`).join('\n')}`
    : ''

  // åˆ†æå¼€å¤´æ–¹å¼
  const opening = analysis.openingPatterns
  const openingTotal = opening.patterns.question + opening.patterns.story + opening.patterns.statement
  let openingStyle = ''
  if (opening.patterns.question > openingTotal * 0.3) {
    openingStyle = 'å¸¸ç”¨é—®å¥å¼€å¤´ï¼Œå¼•å‘è¯»è€…æ€è€ƒ'
    if (opening.examples.question.length > 0) {
      openingStyle += `\n   ä¾‹å¦‚ï¼š"${opening.examples.question[0]}..."`
    }
  } else if (opening.patterns.story > openingTotal * 0.3) {
    openingStyle = 'å–œæ¬¢ç”¨æ•…äº‹æˆ–åœºæ™¯å¼€å¤´ï¼Œè¥é€ ä»£å…¥æ„Ÿ'
    if (opening.examples.story.length > 0) {
      openingStyle += `\n   ä¾‹å¦‚ï¼š"${opening.examples.story[0]}..."`
    }
  } else {
    openingStyle = 'ä¹ æƒ¯å¼€é—¨è§å±±ï¼Œç›´æ¥é™ˆè¿°è§‚ç‚¹'
    if (opening.examples.statement.length > 0) {
      openingStyle += `\n   ä¾‹å¦‚ï¼š"${opening.examples.statement[0]}..."`
    }
  }

  // åˆ†æè½¬æŠ˜è¯ä½¿ç”¨
  const topTransitions = analysis.transitions.slice(0, 3).map(([word, count]) => word).join('ã€')

  return `
ã€å†™ä½œé£æ ¼æ¡£æ¡ˆã€‘

1ï¸âƒ£ è¯­è¨€é£æ ¼
- æ•´ä½“è¯­æ°”ï¼š${toneMap[analysis.tone] || 'ä¸­æ€§å®¢è§‚'}
- å¥å¼ç‰¹ç‚¹ï¼š${sentenceLengthStyle}ï¼ˆå¹³å‡ ${analysis.avgSentenceLength} å­—/å¥ï¼‰
- å¥å¼å¤æ‚åº¦ï¼š${complexityMap[analysis.complexity.diversity]}
  Â· ç®€å•å¥ï¼š${analysis.complexity.simple}%
  Â· å¤åˆå¥ï¼š${analysis.complexity.compound}%
  Â· å¤æ‚å¥ï¼š${analysis.complexity.complex}%
- æ ‡ç‚¹é£æ ¼ï¼š${punctuationStyle}

2ï¸âƒ£ å™è¿°è§†è§’
- ${perspectiveMap[analysis.perspective.dominant]}
- äººç§°åˆ†å¸ƒï¼šæˆ‘(${analysis.perspective.firstPerson}%) / ä½ (${analysis.perspective.secondPerson}%) / ä»–(${analysis.perspective.thirdPerson}%)

3ï¸âƒ£ è¡Œæ–‡ä¹ æƒ¯
- å¼€å¤´æ–¹å¼ï¼š${openingStyle}
- å¸¸ç”¨è½¬æŠ˜è¯ï¼š${topTransitions}
- ä¸»é¢˜åå¥½ï¼š${themeWords}

4ï¸âƒ£ è¡¨è¾¾ç‰¹å¾
${punctuationStyle.includes('æ„Ÿå¹å·') ? '- æƒ…æ„Ÿè¡¨è¾¾ç›´æ¥ï¼Œä¸å›é¿ä¸»è§‚æ„Ÿå—' : ''}
${punctuationStyle.includes('é—®å¥') ? '- å–œæ¬¢é€šè¿‡æé—®å¼•å¯¼æ€è€ƒï¼Œä¸è¯»è€…å»ºç«‹å¯¹è¯æ„Ÿ' : ''}
${analysis.perspective.dominant === 'first' ? '- å¼ºè°ƒä¸ªäººç»å†å’Œä¸»è§‚æ„Ÿå—ï¼ŒçœŸå®æ„Ÿå¼º' : ''}
${analysis.perspective.dominant === 'second' ? '- ç›´æ¥å¯¹è¯è¯»è€…ï¼Œäº’åŠ¨æ€§å¼º' : ''}
${analysis.complexity.diversity === 'complex' ? '- å–„äºä½¿ç”¨å¤æ‚å¥å¼ï¼Œå±‚å±‚é€’è¿›è¡¨è¾¾è§‚ç‚¹' : '- å¥å­ç®€æ´æœ‰åŠ›ï¼Œä¸€é’ˆè§è¡€'}
${topPhrases.length > 3 ? `- æœ‰æ ‡å¿—æ€§çš„è¡¨è¾¾ä¹ æƒ¯ï¼Œå½¢æˆä¸ªäººè¯­è¨€é£æ ¼` : ''}
${expressionExamples}

5ï¸âƒ£ æ¨¡ä»¿æŒ‡å—
å½“ä½ ç”¨è¿™ç§æ–‡é£å†™ä½œæ—¶ï¼š

**æ€ç»´æ–¹å¼**
${opening.patterns.question > openingTotal * 0.2 ? '- å¯ä»¥ç”¨é—®é¢˜å¼•å…¥è¯é¢˜ï¼Œå¼•å‘æ€è€ƒ' : ''}
${analysis.perspective.dominant === 'first' ? '- ä»¥ç¬¬ä¸€äººç§°è§†è§’ï¼Œåˆ†äº«ä¸ªäººç»å†å’Œæ„Ÿå—' : ''}
${analysis.perspective.dominant === 'second' ? '- ç”¨ç¬¬äºŒäººç§°ç›´æ¥å¯¹è¯ï¼Œå¢å¼ºå‚ä¸æ„Ÿ' : ''}

**è¯­è¨€ä¹ æƒ¯**
- ä¿æŒ ${toneMap[analysis.tone]} çš„è¯­æ°”ï¼Œä¸è¦è¿‡äºæ­£å¼æˆ–éšæ„
- å¥å­é•¿åº¦æ§åˆ¶åœ¨ ${Math.max(10, analysis.avgSentenceLength - 5)}-${analysis.avgSentenceLength + 5} å­—ä¹‹é—´
- ${punctuationStyle.includes('æ„Ÿå¹å·') ? 'é€‚å½“ä½¿ç”¨æ„Ÿå¹å·è¡¨è¾¾æ€åº¦' : 'æ ‡ç‚¹ä¿æŒå…‹åˆ¶ï¼Œå°‘ç”¨æ„Ÿå¹å·'}
- ${punctuationStyle.includes('é—®å¥') ? 'å¯ä»¥ç”¨åé—®å’Œè®¾é—®å¢å¼ºäº’åŠ¨æ„Ÿ' : 'ä»¥é™ˆè¿°å¥ä¸ºä¸»'}
- å¸¸ç”¨è¿™äº›è½¬æŠ˜è¯ï¼š${topTransitions}

**è¡¨è¾¾ç»†èŠ‚**
- å…³æ³¨ "${themeWords}" è¿™äº›æ ¸å¿ƒä¸»é¢˜
${topPhrases.length > 0 ? `- å°è¯•ä½¿ç”¨è¿™äº›ç‰¹è‰²è¡¨è¾¾ï¼š${topPhrases.slice(0, 3).join('ã€')}` : ''}
${opening.examples.question.length > 0 || opening.examples.story.length > 0 || opening.examples.statement.length > 0 ? `- å¼€å¤´æ–¹å¼è¦åƒï¼š${opening.examples.question[0] || opening.examples.story[0] || opening.examples.statement[0]}...` : ''}

ğŸ“Š æ•°æ®åŸºç¡€ï¼šåŸºäº ${analysis.totalWords.toLocaleString()} å­—çš„æ–‡æœ¬åˆ†æ
  `.trim()
}
