// æ–‡é£åˆ†æå·¥å…·

// æ¸…æ´—å†…å®¹ - ç§»é™¤æ— ç”¨çš„ç³»ç»Ÿæ–‡å­—å’Œå™ªéŸ³
export const cleanContent = (text) => {
  if (!text) return ''

  let cleaned = text

  // ç§»é™¤å¸¸è§çš„å…¬ä¼—å·ç³»ç»Ÿæç¤º
  const noisePatterns = [
    /ç‚¹å‡»ä¸Šæ–¹.*?å…³æ³¨/g,
    /ç‚¹å‡».*?è“å­—.*?å…³æ³¨/g,
    /å…³æ³¨.*?å…¬ä¼—å·/g,
    /é•¿æŒ‰.*?äºŒç»´ç .*?å…³æ³¨/g,
    /æ‰«æ.*?äºŒç»´ç .*?å…³æ³¨/g,
    /æ¨èé˜…è¯»/g,
    /å¾€æœŸ.*?å›é¡¾/g,
    /ç‚¹å‡».*?é˜…è¯»åŸæ–‡/g,
    /é˜…è¯»åŸæ–‡/g,
    /åŸæ–‡é“¾æ¥/g,
    /è½¬è½½è¯·æ³¨æ˜å‡ºå¤„/g,
    /ç‰ˆæƒå£°æ˜/g,
    /å…è´£å£°æ˜/g,
    /å•†åŠ¡åˆä½œ/g,
    /æŠ•ç¨¿é‚®ç®±/g,
    /è”ç³».*?å¾®ä¿¡/g,
    /æ·»åŠ .*?å¾®ä¿¡/g,
    /æ–‡ç« .*?æ¥æº/g,
    /.*?æ•´ç†.*?ç¼–è¾‘/g,
    /ç‚¹èµ.*?åœ¨çœ‹/g,
    /åˆ†äº«.*?ç‚¹èµ/g,
    /å–œæ¬¢.*?ç‚¹.*?åœ¨çœ‹/g,
    /é¢„è§ˆæ—¶æ ‡ç­¾ä¸å¯ç‚¹/g,
    /å¾®ä¿¡æ‰«ä¸€æ‰«/g,
    /ä½¿ç”¨å°ç¨‹åº/g,
    /å–æ¶ˆ/g,
    /å…è®¸/g,
    /å³å°†æ‰“å¼€.*?å°ç¨‹åº/g,
  ]

  noisePatterns.forEach(pattern => {
    cleaned = cleaned.replace(pattern, '')
  })

  // ç§»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™æ®µè½ç»“æ„ï¼‰
  cleaned = cleaned.replace(/\n\s*\n\s*\n/g, '\n\n')

  // ç§»é™¤é¦–å°¾ç©ºç™½
  cleaned = cleaned.trim()

  // ç§»é™¤è¿‡çŸ­çš„è¡Œï¼ˆå¯èƒ½æ˜¯ç³»ç»Ÿæ–‡å­—ï¼Œå°‘äº5ä¸ªå­—ï¼‰
  cleaned = cleaned.split('\n')
    .filter(line => {
      const trimmed = line.trim()
      // ä¿ç•™ç©ºè¡Œï¼ˆæ®µè½åˆ†éš”ï¼‰æˆ–é•¿åº¦>=5çš„è¡Œ
      return trimmed.length === 0 || trimmed.length >= 5
    })
    .join('\n')

  return cleaned
}

// åˆ†è¯ - ç®€å•å®ç°ï¼ˆä¸­æ–‡æŒ‰å­—ç¬¦ï¼Œè‹±æ–‡æŒ‰å•è¯ï¼‰
export const tokenize = (text) => {
  // ç§»é™¤å¤šä½™ç©ºç™½
  text = text.replace(/\s+/g, ' ').trim()

  // åˆ†ç¦»ä¸­æ–‡å’Œè‹±æ–‡
  const tokens = []
  const chineseRegex = /[\u4e00-\u9fa5]/
  const words = text.split(/\s+/)

  words.forEach(word => {
    if (chineseRegex.test(word)) {
      // ä¸­æ–‡ï¼šæŒ‰å­—ç¬¦åˆ†è¯
      tokens.push(...word.split('').filter(c => chineseRegex.test(c)))
    } else {
      // è‹±æ–‡ï¼šä¿æŒå•è¯
      tokens.push(word.toLowerCase())
    }
  })

  return tokens
}

// æå–å…³é”®è¯ - åŸºäºè¯é¢‘
export const extractKeywords = (texts, topN = 20) => {
  const allText = texts.join(' ')
  const tokens = tokenize(allText)

  // æ‰©å±•çš„åœç”¨è¯åˆ—è¡¨
  const stopWords = new Set([
    // åŸºç¡€åœç”¨è¯
    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™',
    'ä¸ª', 'ä»¬', 'ä¸º', 'èƒ½', 'ä»–', 'å¥¹', 'å®ƒ', 'å¤š', 'æ¥', 'å¹´', 'å¯¹', 'ä¸', 'åŠ', 'ä»¥', 'ç­‰', 'ä½†', 'æˆ–', 'è€Œ', 'ä¸­', 'é‡Œ', 'ä¸‹', 'å¤§', 'å°', 'ä¹ˆ', 'ç»™', 'ä»', 'æŠŠ', 'è¢«', 'è®©',
    'å‡º', 'å¯', 'ç”¨', 'æˆ', 'å› ', 'ä½œ', 'æ›´', 'è¿‡', 'è¿˜', 'ä¹‹', 'æ‰€', 'å¦‚', 'å…¶', 'åª', 'ä¸¤', 'ä¸‰', 'äº›', 'æœ€', 'å·²', 'äº', 'æ—¶', 'å', 'å‰', 'é—´', 'å¤©', 'æœˆ', 'æ—¥', 'æƒ³', 'å¾—',
    'ç§', 'ç‚¹', 'æ–¹', 'é¢', 'æ¬¡', 'ç°', 'å…³', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'å·²ç»', 'è¿˜æ˜¯', 'å¯ä»¥', 'è¿™ä¸ª', 'é‚£ä¸ª', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'çš„è¯', 'å¦‚æœ', 'è™½ç„¶',
    // å…¬ä¼—å·å¸¸è§è¯
    'æ–‡ç« ', 'å†…å®¹', 'é˜…è¯»', 'å…³æ³¨', 'åˆ†äº«', 'ç‚¹å‡»', 'æ‰«ç ', 'äºŒç»´ç ', 'å¾®ä¿¡', 'å…¬ä¼—å·', 'æ¨è', 'ç²¾å½©', 'åŸåˆ›', 'è½¬è½½', 'æ¥æº', 'ç¼–è¾‘', 'æ•´ç†', 'æ’ç‰ˆ', 'å›¾ç‰‡', 'è§†é¢‘',
    // æ•°å­—
    'ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹', 'å',
    // è‹±æ–‡åœç”¨è¯
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those', 'it', 'its', 'they', 'them', 'their'
  ])

  // ç»Ÿè®¡è¯é¢‘
  const freq = {}
  tokens.forEach(token => {
    if (!stopWords.has(token) && token.length > 1) {
      freq[token] = (freq[token] || 0) + 1
    }
  })

  // æ’åºå¹¶è¿”å›å‰ N ä¸ª
  return Object.entries(freq)
    .sort((a, b) => b[1] - a[1])
    .slice(0, topN)
    .map(([word, count]) => ({ word, count }))
}

// æå–å¸¸ç”¨çŸ­è¯­ï¼ˆ2-3 å­—/è¯ï¼‰
export const extractCommonPhrases = (texts, topN = 10) => {
  const allText = texts.join(' ')
  const sentences = allText.split(/[ã€‚ï¼ï¼Ÿ\n.!?]/).filter(s => s.trim())

  const phrases = {}
  const chineseRegex = /[\u4e00-\u9fa5]/

  sentences.forEach(sentence => {
    if (chineseRegex.test(sentence)) {
      // ä¸­æ–‡ï¼šæå– 2-3 å­—è¯ç»„
      for (let i = 0; i < sentence.length - 1; i++) {
        for (let len = 2; len <= 3 && i + len <= sentence.length; len++) {
          const phrase = sentence.slice(i, i + len)
          if (phrase.trim().length === len) {
            phrases[phrase] = (phrases[phrase] || 0) + 1
          }
        }
      }
    } else {
      // è‹±æ–‡ï¼šæå– 2-3 è¯ç»„
      const words = sentence.split(/\s+/).filter(w => w.length > 0)
      for (let i = 0; i < words.length - 1; i++) {
        for (let len = 2; len <= 3 && i + len <= words.length; len++) {
          const phrase = words.slice(i, i + len).join(' ')
          phrases[phrase] = (phrases[phrase] || 0) + 1
        }
      }
    }
  })

  return Object.entries(phrases)
    .filter(([_, count]) => count >= 2) // è‡³å°‘å‡ºç° 2 æ¬¡
    .sort((a, b) => b[1] - a[1])
    .slice(0, topN)
    .map(([phrase, count]) => ({ phrase, count }))
}

// è®¡ç®—å¹³å‡å¥å­é•¿åº¦
export const calculateAvgSentenceLength = (texts) => {
  const allText = texts.join(' ')
  const sentences = allText.split(/[ã€‚ï¼ï¼Ÿ\n.!?]/).filter(s => s.trim())

  if (sentences.length === 0) return 0

  const totalLength = sentences.reduce((sum, s) => sum + s.trim().length, 0)
  return Math.round(totalLength / sentences.length)
}

// åˆ†ææ ‡ç‚¹ç¬¦å·ä½¿ç”¨ä¹ æƒ¯
export const analyzePunctuation = (texts) => {
  const allText = texts.join(' ')
  const punctuations = {
    'ã€‚': 0, 'ï¼': 0, 'ï¼Ÿ': 0, 'ï¼Œ': 0, 'ï¼›': 0, 'ï¼š': 0,
    '.': 0, '!': 0, '?': 0, ',': 0, ';': 0, ':': 0,
    'ã€': 0, 'â€¦': 0, 'â€”': 0, '"': 0, '"': 0, '\u2018': 0, '\u2019': 0
  }

  for (const char of allText) {
    if (char in punctuations) {
      punctuations[char]++
    }
  }

  return punctuations
}

// æ£€æµ‹è¯­æ°”/é£æ ¼
export const detectTone = (texts) => {
  const allText = texts.join(' ')

  // ç®€å•çš„è§„åˆ™æ£€æµ‹
  const casual = /å“ˆå“ˆ|å˜¿|å–‚|å“|å•Š|å‘€|å˜›|å§|å‘¢|å“¦|emmm|hhh/gi
  const formal = /å› æ­¤|æ‰€ä»¥|ç»¼ä¸Š|æ€»ä¹‹|ä»è€Œ|é‰´äº|åŸºäº/gi
  const humorous = /ç¬‘|å“­|å°´å°¬|æç¬‘|æœ‰è¶£|å“ˆå“ˆ/gi

  const casualCount = (allText.match(casual) || []).length
  const formalCount = (allText.match(formal) || []).length
  const humorousCount = (allText.match(humorous) || []).length

  if (casualCount > formalCount && casualCount > humorousCount) {
    return 'casual'
  } else if (formalCount > casualCount && formalCount > humorousCount) {
    return 'formal'
  } else if (humorousCount > 0) {
    return 'humorous'
  } else {
    return 'neutral'
  }
}

// ä¸»åˆ†æå‡½æ•°
export const analyzeWritingStyle = (sources) => {
  if (!sources || sources.length === 0) {
    return null
  }

  const texts = sources.map(s => s.content).filter(Boolean)

  if (texts.length === 0) {
    return null
  }

  const keywords = extractKeywords(texts)
  const commonPhrases = extractCommonPhrases(texts)
  const avgSentenceLength = calculateAvgSentenceLength(texts)
  const punctuationStyle = analyzePunctuation(texts)
  const tone = detectTone(texts)
  const totalWords = texts.reduce((sum, text) => sum + text.length, 0)

  return {
    keywords,
    commonPhrases,
    avgSentenceLength,
    punctuationStyle,
    tone,
    totalWords,
    analyzedAt: new Date().toISOString()
  }
}

// ç”Ÿæˆæ–‡é£æè¿°ï¼ˆç”¨äº AI promptï¼‰
export const generateStyleDescription = (analysis) => {
  if (!analysis) {
    return ''
  }

  const toneMap = {
    casual: 'è½»æ¾éšæ„',
    formal: 'æ­£å¼ä¸¥è°¨',
    humorous: 'å¹½é»˜é£è¶£',
    neutral: 'ä¸­æ€§å®¢è§‚'
  }

  // åˆ†æå¥å­é•¿åº¦åå¥½
  const sentenceLengthStyle = analysis.avgSentenceLength < 15
    ? 'åå¥½ä½¿ç”¨çŸ­å¥ï¼ŒèŠ‚å¥æ˜å¿«ï¼Œé€‚åˆè¡¨è¾¾æ¸…æ™°çš„è§‚ç‚¹'
    : analysis.avgSentenceLength < 25
    ? 'å¥å­é•¿åº¦é€‚ä¸­ï¼Œå…¼é¡¾è¡¨è¾¾æ·±åº¦å’Œé˜…è¯»æµç•…æ€§'
    : 'å€¾å‘ä½¿ç”¨é•¿å¥ï¼Œå–„äºé“ºé™ˆå’Œç»†èŠ‚æå†™ï¼Œè¡¨è¾¾æ›´æœ‰å±‚æ¬¡æ„Ÿ'

  // åˆ†ææ ‡ç‚¹ä½¿ç”¨ä¹ æƒ¯
  const punc = analysis.punctuationStyle
  const totalPunc = Object.values(punc).reduce((sum, count) => sum + count, 0)
  const exclamationRatio = ((punc['ï¼'] + punc['!']) / totalPunc * 100).toFixed(1)
  const questionRatio = ((punc['ï¼Ÿ'] + punc['?']) / totalPunc * 100).toFixed(1)
  const ellipsisRatio = ((punc['â€¦'] || 0) / totalPunc * 100).toFixed(1)

  let punctuationStyle = ''
  if (parseFloat(exclamationRatio) > 5) {
    punctuationStyle += 'ç»å¸¸ä½¿ç”¨æ„Ÿå¹å·ï¼Œè¡¨è¾¾æƒ…æ„Ÿä¸°å¯Œã€æ€åº¦é²œæ˜ï¼›'
  }
  if (parseFloat(questionRatio) > 3) {
    punctuationStyle += 'å–„ç”¨é—®å¥ä¸è¯»è€…äº’åŠ¨ï¼Œå¼•å‘æ€è€ƒï¼›'
  }
  if (parseFloat(ellipsisRatio) > 2) {
    punctuationStyle += 'ä½¿ç”¨çœç•¥å·è¥é€ ç•™ç™½å’Œæ€è€ƒç©ºé—´ï¼›'
  }
  if (!punctuationStyle) {
    punctuationStyle = 'æ ‡ç‚¹ä½¿ç”¨å…‹åˆ¶ï¼Œä»¥é™ˆè¿°ä¸ºä¸»ï¼Œè¯­æ°”å¹³ç¨³ã€‚'
  }

  // æå–å…³é”®ä¸»é¢˜è¯ï¼ˆæ’é™¤é€šç”¨è¯åçš„é«˜é¢‘è¯ï¼‰
  const topKeywords = analysis.keywords.slice(0, 10).map(k => k.word)
  const themeWords = topKeywords.slice(0, 5).join('ã€')

  // æå–å¸¸ç”¨è¡¨è¾¾ï¼ˆçŸ­è¯­ï¼‰
  const topPhrases = analysis.commonPhrases.slice(0, 8).map(p => p.phrase)
  const expressionExamples = topPhrases.length > 0
    ? `\n\nã€å¸¸ç”¨è¡¨è¾¾æ–¹å¼ã€‘\n${topPhrases.map((p, i) => `${i + 1}. "${p}"`).join('\n')}`
    : ''

  return `
ã€å†™ä½œé£æ ¼æ¡£æ¡ˆã€‘

1ï¸âƒ£ è¯­è¨€é£æ ¼
- æ•´ä½“è¯­æ°”ï¼š${toneMap[analysis.tone] || 'ä¸­æ€§å®¢è§‚'}
- å¥å¼ç‰¹ç‚¹ï¼š${sentenceLengthStyle}ï¼ˆå¹³å‡ ${analysis.avgSentenceLength} å­—/å¥ï¼‰
- æ ‡ç‚¹é£æ ¼ï¼š${punctuationStyle}

2ï¸âƒ£ ä¸»é¢˜åå¥½
ç»å¸¸æ¢è®¨çš„è¯é¢˜å…³é”®è¯ï¼š${themeWords}

3ï¸âƒ£ è¡¨è¾¾ä¹ æƒ¯
${punctuationStyle.includes('æ„Ÿå¹å·') ? '- æƒ…æ„Ÿè¡¨è¾¾ç›´æ¥ï¼Œä¸å›é¿ä¸»è§‚æ„Ÿå—' : ''}
${punctuationStyle.includes('é—®å¥') ? '- å–œæ¬¢é€šè¿‡æé—®å¼•å¯¼æ€è€ƒï¼Œä¸è¯»è€…å»ºç«‹å¯¹è¯æ„Ÿ' : ''}
${analysis.avgSentenceLength > 25 ? '- å–„äºä½¿ç”¨å¤æ‚å¥å¼ï¼Œå±‚å±‚é€’è¿›è¡¨è¾¾è§‚ç‚¹' : '- å¥å­ç®€æ´æœ‰åŠ›ï¼Œä¸€é’ˆè§è¡€'}
${topPhrases.length > 3 ? `- æœ‰æ ‡å¿—æ€§çš„è¡¨è¾¾ä¹ æƒ¯ï¼Œå½¢æˆä¸ªäººè¯­è¨€é£æ ¼` : ''}
${expressionExamples}

4ï¸âƒ£ å†™ä½œå»ºè®®
å½“ä½ æ¨¡ä»¿è¿™ç§æ–‡é£æ—¶ï¼š
- ä¿æŒ ${toneMap[analysis.tone]} çš„è¯­æ°”ï¼Œä¸è¦è¿‡äºæ­£å¼æˆ–éšæ„
- å¥å­é•¿åº¦æ§åˆ¶åœ¨ ${Math.max(10, analysis.avgSentenceLength - 5)}-${analysis.avgSentenceLength + 5} å­—ä¹‹é—´
- ${punctuationStyle.includes('æ„Ÿå¹å·') ? 'é€‚å½“ä½¿ç”¨æ„Ÿå¹å·è¡¨è¾¾æ€åº¦' : 'æ ‡ç‚¹ä¿æŒå…‹åˆ¶ï¼Œå°‘ç”¨æ„Ÿå¹å·'}
- ${punctuationStyle.includes('é—®å¥') ? 'å¯ä»¥ç”¨åé—®å’Œè®¾é—®å¢å¼ºäº’åŠ¨æ„Ÿ' : 'ä»¥é™ˆè¿°å¥ä¸ºä¸»'}
- å…³æ³¨ "${themeWords}" è¿™äº›æ ¸å¿ƒä¸»é¢˜
${topPhrases.length > 0 ? `- å°è¯•ä½¿ç”¨è¿™äº›ç‰¹è‰²è¡¨è¾¾ï¼š${topPhrases.slice(0, 3).join('ã€')}` : ''}

ğŸ“Š æ•°æ®åŸºç¡€ï¼šåŸºäº ${analysis.totalWords.toLocaleString()} å­—çš„æ–‡æœ¬åˆ†æ
  `.trim()
}
